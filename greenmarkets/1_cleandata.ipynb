{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing Recipes into Data \n",
    "\n",
    "### Takes recipes from both BigOven and Spoonacular API and puts them into data formats useable in modeling. Needs to be rerun after pulling more recipes \n",
    "\n",
    "##### Includes all deduping\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/bridgitboulahanis/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from pathlib import Path\n",
    "import string\n",
    "import re\n",
    "import pickle\n",
    "from itertools import chain\n",
    "from fuzzywuzzy import fuzz, process\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "from itertools import chain\n",
    "from collections import Counter\n",
    "import nltk\n",
    "from pathlib import Path\n",
    "\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/bridgitboulahanis/Dropbox/Not_work/insight/greenmarkets/bigoven/maindish/maindish-131-68-87332.json\n",
      "/Users/bridgitboulahanis/Dropbox/Not_work/insight/greenmarkets/bigoven/maindish/maindish-131-71-87778.json\n",
      "/Users/bridgitboulahanis/Dropbox/Not_work/insight/greenmarkets/bigoven/maindish/maindish-181-96-1158777.json\n",
      "/Users/bridgitboulahanis/Dropbox/Not_work/insight/greenmarkets/bigoven/maindish/maindish-164-249-1283792.json\n",
      "/Users/bridgitboulahanis/Dropbox/Not_work/insight/greenmarkets/bigoven/maindish/maindish-205-249-1797822.json\n",
      "/Users/bridgitboulahanis/Dropbox/Not_work/insight/greenmarkets/bigoven/maindish/maindish-55-19-156227.json\n",
      "/Users/bridgitboulahanis/Dropbox/Not_work/insight/greenmarkets/bigoven/maindish/maindish-127-74-2431270.json\n",
      "/Users/bridgitboulahanis/Dropbox/Not_work/insight/greenmarkets/bigoven/maindish/maindish-112-159-862538.json\n",
      "/Users/bridgitboulahanis/Dropbox/Not_work/insight/greenmarkets/bigoven/maindish/maindish-55-62-166860.json\n",
      "/Users/bridgitboulahanis/Dropbox/Not_work/insight/greenmarkets/bigoven/maindish/maindish-152-176-1770269.json\n",
      "/Users/bridgitboulahanis/Dropbox/Not_work/insight/greenmarkets/bigoven/maindish/maindish-35-191-157959.json\n",
      "/Users/bridgitboulahanis/Dropbox/Not_work/insight/greenmarkets/bigoven/maindish/maindish-185-162-1234944.json\n",
      "/Users/bridgitboulahanis/Dropbox/Not_work/insight/greenmarkets/bigoven/maindish/maindish-131-76-83346.json\n",
      "/Users/bridgitboulahanis/Dropbox/Not_work/insight/greenmarkets/bigoven/maindish/maindish-74-82-565669.json\n",
      "/Users/bridgitboulahanis/Dropbox/Not_work/insight/greenmarkets/bigoven/maindish/maindish-51-46-372714.json\n",
      "/Users/bridgitboulahanis/Dropbox/Not_work/insight/greenmarkets/bigoven/maindish/maindish-108-181-2437430.json\n",
      "/Users/bridgitboulahanis/Dropbox/Not_work/insight/greenmarkets/bigoven/maindish/maindish-13-123-101172.json\n",
      "/Users/bridgitboulahanis/Dropbox/Not_work/insight/greenmarkets/bigoven/maindish/maindish-249-246-1189094.json\n",
      "/Users/bridgitboulahanis/Dropbox/Not_work/insight/greenmarkets/bigoven/maindish/maindish-93-94-1189449.json\n",
      "/Users/bridgitboulahanis/Dropbox/Not_work/insight/greenmarkets/bigoven/maindish/maindish-140-74-1548281.json\n",
      "/Users/bridgitboulahanis/Dropbox/Not_work/insight/greenmarkets/bigoven/maindish/maindish-89-172-1157050.json\n",
      "/Users/bridgitboulahanis/Dropbox/Not_work/insight/greenmarkets/bigoven/maindish/maindish-229-249-1922931.json\n",
      "/Users/bridgitboulahanis/Dropbox/Not_work/insight/greenmarkets/bigoven/maindish/maindish-226-90-1782293.json\n",
      "/Users/bridgitboulahanis/Dropbox/Not_work/insight/greenmarkets/bigoven/maindish/maindish-160-248-2214799.json\n",
      "/Users/bridgitboulahanis/Dropbox/Not_work/insight/greenmarkets/bigoven/maindish/maindish-131-74-94994.json\n",
      "/Users/bridgitboulahanis/Dropbox/Not_work/insight/greenmarkets/bigoven/maindish/maindish-35-188-169625.json\n",
      "/Users/bridgitboulahanis/Dropbox/Not_work/insight/greenmarkets/bigoven/maindish/maindish-78-21-723182.json\n",
      "/Users/bridgitboulahanis/Dropbox/Not_work/insight/greenmarkets/bigoven/maindish/maindish-97-51-2267070.json\n",
      "/Users/bridgitboulahanis/Dropbox/Not_work/insight/greenmarkets/bigoven/maindish/maindish-161-13-2189043.json\n",
      "/Users/bridgitboulahanis/Dropbox/Not_work/insight/greenmarkets/bigoven/maindish/maindish-120-7-175607.json\n",
      "/Users/bridgitboulahanis/Dropbox/Not_work/insight/greenmarkets/bigoven/maindish/maindish-24-142-173069.json\n",
      "/Users/bridgitboulahanis/Dropbox/Not_work/insight/greenmarkets/bigoven/maindish/maindish-81-243-208379.json\n",
      "/Users/bridgitboulahanis/Dropbox/Not_work/insight/greenmarkets/bigoven/maindish/maindish-131-67-101539.json\n",
      "/Users/bridgitboulahanis/Dropbox/Not_work/insight/greenmarkets/bigoven/maindish/maindish-43-73-732187.json\n",
      "/Users/bridgitboulahanis/Dropbox/Not_work/insight/greenmarkets/bigoven/maindish/maindish-222-94-1999598.json\n",
      "/Users/bridgitboulahanis/Dropbox/Not_work/insight/greenmarkets/bigoven/maindish/maindish-173-37-1925632.json\n",
      "/Users/bridgitboulahanis/Dropbox/Not_work/insight/greenmarkets/bigoven/maindish/maindish-31-245-171873.json\n",
      "/Users/bridgitboulahanis/Dropbox/Not_work/insight/greenmarkets/bigoven/maindish/maindish-58-230-159896.json\n",
      "/Users/bridgitboulahanis/Dropbox/Not_work/insight/greenmarkets/bigoven/maindish/maindish-85-210-1153699.json\n",
      "/Users/bridgitboulahanis/Dropbox/Not_work/insight/greenmarkets/bigoven/maindish/maindish-136-74-102089.json\n",
      "/Users/bridgitboulahanis/Dropbox/Not_work/insight/greenmarkets/bigoven/maindish/maindish-17-55-170863.json\n",
      "/Users/bridgitboulahanis/Dropbox/Not_work/insight/greenmarkets/bigoven/maindish/maindish-131-78-84348.json\n",
      "/Users/bridgitboulahanis/Dropbox/Not_work/insight/greenmarkets/bigoven/maindish/maindish-156-236-1705868.json\n",
      "/Users/bridgitboulahanis/Dropbox/Not_work/insight/greenmarkets/bigoven/maindish/maindish-242-46-1668910.json\n",
      "/Users/bridgitboulahanis/Dropbox/Not_work/insight/greenmarkets/bigoven/maindish/maindish-148-128-1880841.json\n",
      "/Users/bridgitboulahanis/Dropbox/Not_work/insight/greenmarkets/bigoven/maindish/maindish-123-131-168303.json\n",
      "/Users/bridgitboulahanis/Dropbox/Not_work/insight/greenmarkets/bigoven/maindish/maindish-104-213-32289.json\n",
      "/Users/bridgitboulahanis/Dropbox/Not_work/insight/greenmarkets/bigoven/maindish/maindish-62-217-387956.json\n",
      "/Users/bridgitboulahanis/Dropbox/Not_work/insight/greenmarkets/bigoven/maindish/maindish-189-186-2446226.json\n",
      "/Users/bridgitboulahanis/Dropbox/Not_work/insight/greenmarkets/bigoven/maindish/maindish-201-249-1357815.json\n",
      "/Users/bridgitboulahanis/Dropbox/Not_work/insight/greenmarkets/bigoven/maindish/maindish-169-27-1342261.json\n",
      "/Users/bridgitboulahanis/Dropbox/Not_work/insight/greenmarkets/bigoven/maindish/maindish-116-87-173133.json\n"
     ]
    }
   ],
   "source": [
    "#first load all recipe data\n",
    "BOrecipes = []\n",
    "BOingredients = []\n",
    "BOids = []\n",
    "BOinstructions = []\n",
    "BOtitle = []\n",
    "BOpath = Path('~/Dropbox/Not_work/insight/greenmarkets/bigoven/maindish/')\n",
    "for i, po in enumerate(BOpath.expanduser().glob('*.json')):\n",
    "    with po.open() as fh:\n",
    "        try:\n",
    "            r = json.load(fh)\n",
    "        except Exception:\n",
    "            print(str(po))\n",
    "            continue\n",
    "        BOrecipes.append(r)\n",
    "        BOing = r.get('Ingredients')\n",
    "        BOid = r.get('RecipeID')\n",
    "        BOinstructions.append(r.get('Instructions'))\n",
    "        BOtitle.append(r.get('Title'))\n",
    "        BOids.append(r.get('RecipeID'))\n",
    "        if BOing is not None:\n",
    "            BOingredients.append([lemmatizer.lemmatize(re.sub(r'[^a-z ]', '', i['Name'].lower().strip()))\n",
    "                                  for i in BOing if i is not None and i['Name'] is not None])\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current recipe count: 62160\n",
      "Current list of ingredient count: 62160\n",
      "Number of ingredients: 691631\n",
      "Unique ingredients: 127577\n"
     ]
    }
   ],
   "source": [
    "print('Current recipe count: ' + str(len(BOinstructions)))\n",
    "print('Current list of ingredient count: ' + str(len(BOingredients)))\n",
    "numing = []\n",
    "numing.extend([len(r) for r in BOingredients])\n",
    "print('Number of ingredients: ' + str(sum(numing)))\n",
    "aa = list(chain.from_iterable(BOingredients))\n",
    "print('Unique ingredients: ' + str(len(Counter(aa).keys())))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now we want to load spoonacular data and create a dataframe that has ingredients and their aisle \n",
    "# this will be our classification data\n",
    "\n",
    "SArecipes = []\n",
    "SAingredients = []\n",
    "SAingclass = []\n",
    "SAnoise = []\n",
    "SAingID = []\n",
    "SAmeta = []\n",
    "SAmeasure = []\n",
    "\n",
    "seenID = set()\n",
    "SApath = Path('~/Dropbox/Not_work/insight/greenmarkets/spoonac/data/')\n",
    "for i, po in enumerate(SApath.expanduser().glob('*/*.json')):\n",
    "    with po.open() as fh:\n",
    "        try:\n",
    "            r = json.load(fh)\n",
    "        except Exception:\n",
    "            print(str(po))\n",
    "            continue\n",
    "        for rec in r:\n",
    "            currenting = rec['extendedIngredients']\n",
    "            for ing in currenting:\n",
    "                SAnoise.extend(i.strip() for i in ing['metaInformation'])\n",
    "                if ing['id'] in seenID:\n",
    "                    continue\n",
    "                seenID.add(ing['id']) \n",
    "                SAingredients.append(ing['name'])\n",
    "                SAingclass.append(ing['aisle'])  \n",
    "                SAingID.append(ing['id'])\n",
    "                SAmeasure.append(ing['measures']['us']['unitLong'])\n",
    "                SAmeta.extend([ing['meta']])\n",
    "\n",
    "                    \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#grab all the ingrdients from spoonacular and put in dataframe\n",
    "classifying = pd.DataFrame({'ingredient': SAingredients, 'aisle': SAingclass, 'meta': SAmeta, 'measure': SAmeasure,  'id': SAingID})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#some words I noticed should get added to SAnoise\n",
    "wta = ['ground', 'taste', 'ounce', 'large']\n",
    "SAnoise.extend(wta)\n",
    "SAnoise = set(SAnoise)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#suite of deduping functions\n",
    "\n",
    "#on loading we tokenize, strip punctuation&digits&spaces and make lowercase \n",
    "#here we are removing meta words and stop words\n",
    "stop_words = set(stopwords.words('english')) \n",
    "\n",
    "\n",
    "def removenoise(input_ing, posdict):\n",
    "    assert type(input_ing) == list, type(input_ing)\n",
    "    noise_free_ing = []\n",
    "    for word in input_ing:\n",
    "        checked = []\n",
    "        splitit = word.split()\n",
    "        for it in splitit:\n",
    "            if it not in SAnoise and it not in stop_words:\n",
    "                tag = nltk.pos_tag([it])\n",
    "                if tag[0][1] in ['JJ', 'NN', 'NNP', 'VBN']: #check if it is a adj, noun, proper noun\n",
    "                    checked.append(it)\n",
    "                  \n",
    "                elif tag[0][1] in ['JJR', 'JJS', 'NNS', 'NNPS']: # check if adj, noun, propr noun, but comparitive or plural\n",
    "                    checked.append(lemmatizer.lemmatize(it)) #lemmatize these words\n",
    "                    \n",
    "                else: #otherwise let's throw those words into a dict just to make sure we aren't stripping any real data\n",
    "                    if tag[0][1] not in posdict:\n",
    "                        posdict[tag[0][1]] = set([it])\n",
    "                    else:\n",
    "                        posdict[tag[0][1]].add(it)\n",
    "        if len(checked) > 0:\n",
    "            noise_free_ing.append(' '.join(list(filter(None, checked))))\n",
    "    return noise_free_ing\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#call the dedupe functions\n",
    "nning = []\n",
    "posdict = {}\n",
    "for i in BOingredients:\n",
    "    nning.append(removenoise(i, posdict))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique ingredients: 127577\n",
      "Unique deduped ingredients: 79200\n"
     ]
    }
   ],
   "source": [
    "print('Unique ingredients: ' + str(len(Counter(list(chain.from_iterable(BOingredients))).keys())))\n",
    "print('Unique deduped ingredients: ' + str(len(Counter(list(chain.from_iterable(nning))).keys())))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classify ingredients into aisles\n",
    "#Note! This is a very slow loop (~40 mins)\n",
    "whichaisle = {}\n",
    "seen = set()\n",
    "notin = []\n",
    "countclassified = 0\n",
    "unclassified = 0\n",
    "for i in nning:\n",
    "    for j in i:\n",
    "        if j in seen:\n",
    "            continue\n",
    "        seen.add(j) \n",
    "        highest = process.extractOne(j,classifying['ingredient']) #using FW to get the closest ingredient match\n",
    "        a = classifying.index[classifying['ingredient'] == highest[0]] #pulling out the index of that ingredient in aisle df\n",
    "        if highest[1] > 80: #set minimum bar for ingredient similarity\n",
    "            whichaisle[j] = classifying['aisle'][a].tolist() # put in aisle classification\n",
    "            countclassified += 1\n",
    "        else:\n",
    "            whichaisle[j]  = None\n",
    "            notin.append(j)\n",
    "            unclassified += 1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#output the data \n",
    "pickle.dump(nning,open(\"eatlocal/generated_data/cleaned_ingredients.pkl\",\"wb\"))\n",
    "pickle.dump(whichaisle,open(\"eatlocal/generated_data/ingredient_aisle.pkl\",\"wb\"))\n",
    "pickle.dump(BOinstructions, open(\"eatlocal/generated_data/instructions.pkl\", \"wb\"))\n",
    "pickle.dump(BOtitle, open(\"eatlocal/generated_data/recipetitle.pkl\", \"wb\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
